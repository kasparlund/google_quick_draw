{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and visualizing the data\n",
    "\n",
    "In the search for a solution ot keep all 50 million images in memory i evaluated:\n",
    "- generating and compressing grasycale images \n",
    "- compressing the drawing\n",
    "\n",
    "\n",
    "\n",
    "Both methods works: \n",
    "- Webp compression for grayscale images is very efficient with a fast decompression. The compression takes about 5h 30 min \n",
    "- Brotli compression of strokes in text format is as efficient as bz2 decompresses. Both compress much better than zlib. Brotli and zlib's decompression is 3 times fast compared to bz2. The compression takes about 10h 30 min (very slow compared to bz2 and zlib). \n",
    "- i have a macbook pro mid 2016\n",
    "\n",
    "Decompression of 50 million images can be generated form the compressed state to images in 13 and 26 min from the compressed drawing and compressed images respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code import *\n",
    "from graphics import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from pickle import *\n",
    "import brotli\n",
    "from core_ai.lib.data import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data             = Path(\"../../data/google_quick_draw\")\n",
    "print(data.exists())\n",
    "input_data       = data #Path(\"/Users/kasparlund/DropBox\")\n",
    "\n",
    "stroke_dir       = input_data/\"train_simplified\"\n",
    "one_stroke_file  = stroke_dir/\"ambulance.csv\"\n",
    "\n",
    "data_prepared    = data/\"train\"\n",
    "data_prepared.mkdir(exist_ok=True)\n",
    "data_compressed  = data/\"compressed\"\n",
    "data_compressed.mkdir(exist_ok=True)\n",
    "\n",
    "#astrokes_file = data_prepared/\"train_simplified.csv\"\n",
    "vocab_file      = data_compressed/\"vocab.json\"\n",
    "\n",
    "train_ds_file = data_prepared/\"train_dataset.pkl\"\n",
    "valid_ds_file = data_prepared/\"valid_dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimages_file      = data_prepared/f\"imgs_webp_{size}x{size}_lw{line_width}.pkl\"\\ntrain_valid_name = \"c_drawing_train_valid\" if compressed_drawings else                     f\"c_image_train_valid_{size}x{size}_lw{line_width}.gzip\"\\ntrain_valid_file = data_prepared/f\"{train_valid_name}.gzip\"\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size, line_width  = 28, 1\n",
    "valid_ratio = 0.1\n",
    "linetype    = cv2.LINE_8 #cv2.LINE_AA\n",
    "compressed_drawings = True\n",
    "\n",
    "drawings_file    = data_prepared/f\"c_drawings.pkl\"\n",
    "\"\"\"\n",
    "images_file      = data_prepared/f\"imgs_webp_{size}x{size}_lw{line_width}.pkl\"\n",
    "train_valid_name = \"c_drawing_train_valid\" if compressed_drawings else \\\n",
    "                    f\"c_image_train_valid_{size}x{size}_lw{line_width}.gzip\"\n",
    "train_valid_file = data_prepared/f\"{train_valid_name}.gzip\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_compressed.exists()), print(data.exists()), print(one_stroke_file.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyses one data tabel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#read stroke data\n",
    "df = pd.read_csv(one_stroke_file)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[ pd.isna(df['word']) ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df[\"word\"] = df[\"word\"].apply( lambda word: word.strip().replace(\" \", \"_\") )\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import brotli\n",
    "s_drawing = df.drawing.copy()\n",
    "\n",
    "%time s_drawing = s_drawing.apply( lambda txt: brotli.compress(txt.encode(\"utf-8\") ) )\n",
    "\n",
    "%time drawing = brotli.decompress( s_drawing[0] ).decode(\"utf-8\")\n",
    "\n",
    "print(len(df.drawing[0]), len(s_drawing[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ix            = [i for i in np.random.randint(0,len(df),1000) ]\n",
    "%time imgs_de = drawings2images( df.drawing[ix], size, line_width, linetype, compressed_drawings = False )\n",
    "%time imgs_de = drawings2images( s_drawing[ix],  size, line_width, linetype, compressed_drawings = compressed_drawings)\n",
    "plot_images( imgs_de, cm.Greys, max_rows_cols=8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "- compress all images or drawings into lists of compressed drawing or images, \n",
    "- create one annotation file\n",
    "- replace space in labels with _\n",
    "- create vocabulary file to tranlate between label and numeric tokens\n",
    "\n",
    "- create a traiing and validation dataframe with the minimum number of required columns from the annotation file and the compressed image or drawing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: airplane.csv------------------------------------------------------------------------------| 0.00% [0/340 00:00<00:00]\n",
      "processing: alarm clock.csv---------------------------------------------------------------------------| 0.29% [1/340 01:31<8:38:25]\n",
      "processing: ambulance.csv-----------------------------------------------------------------------------| 0.59% [2/340 03:13<9:04:32]\n",
      "processing: angel.csv---------------------------------------------------------------------------------| 0.88% [3/340 05:17<9:53:36]\n"
     ]
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time\n",
    "\n",
    "import pickle\n",
    "def merge_stroke_files(stroke_dir:Path, vocab_file:Path, data_compressed:Path, compressed_drawings:bool ):\n",
    "    nrows = 0\n",
    "    vocab = {}\n",
    "    token = 0\n",
    "    files = list( stroke_dir.glob(\"*.csv\") )\n",
    "    for f in  progress_bar(files):\n",
    "        gc.collect()\n",
    "        print(f\"processing: {f.name}\")\n",
    "        df = pd.read_csv(f)\n",
    "        #df.drop([\"timestamp\"], axis=1, inplace=True)\n",
    "                \n",
    "        #replace space in label with _, add label to vocab and create word_code column\n",
    "        if sum(pd.isna(df['word'])) > 0: print(f\"nan in word field for file:{f}\")\n",
    "        df[\"word\"] = df[\"word\"].apply( lambda word: word.strip().replace(\" \", \"_\") )\n",
    "        vocab[df[\"word\"][0]] = token\n",
    "        df[\"word_code\"]      = token\n",
    "        df[\"word_code\"] = df[\"word_code\"].astype(np.uint16)\n",
    "        token += 1\n",
    "                \n",
    "        if compressed_drawings:\n",
    "            df[\"drawing\"] = [brotli.compress(txt.encode(\"utf-8\"), quality=11) for txt in df[\"drawing\"] ]\n",
    "        else:    \n",
    "            df[\"drawing\"] = convert_strokes2webp(df[\"drawing\"], size, line_width)\n",
    "            \n",
    "        #include header at first write\n",
    "                  \n",
    "        f_out = (data_compressed/f.name).with_suffix(\".gzip\")\n",
    "        df.to_pickle(f_out, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "                  \n",
    "        #with (data_compressed/f.name).open(\"w\") as fc: df.to_csv(fc, encoding=\"utf-8\")                 \n",
    "        nrows += df.shape[0]\n",
    "        df = None\n",
    "                                           \n",
    "    with open(vocab_file, 'w') as fp:\n",
    "        json.dump(vocab, fp)\n",
    "                                           \n",
    "    print(f\"total number of records: {nrows}\") \n",
    "    print(f\"vocab size:{len(vocab)}:\\n{vocab}\")\n",
    "    gc.collect()\n",
    "\n",
    "if not vocab_file.exists():\n",
    "    if compressed_drawings: \n",
    "        %time merge_stroke_files(stroke_dir, vocab_file, data_compressed, compressed_drawings=compressed_drawings)\n",
    "    else:\n",
    "        %time merge_stroke_files(stroke_dir, vocab_file, images_file,   compressed_drawings=False)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_split( file, columns:list,  valid_ratio:float):\n",
    "    #df = pd.read_csv(f, encoding=\"utf-8\", usecols=columns, dtype=dtype_columns)        \n",
    "    df = pd.read_pickle(file)\n",
    "    \n",
    "    if columns is not None:\n",
    "        drop_cols = [c for c in df.columns if not c in columns]\n",
    "        df.drop(labels=drop_cols, axis=\"columns\", inplace=True)\n",
    "        \n",
    "    ix_valid    = np.random.randint(0, len(df), size=int(len(df)*valid_ratio+.5) )\n",
    "    df[\"train\"] = True\n",
    "    df.iloc[ix_valid, df.columns.get_loc(\"train\")] = False\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_and_split_all( compressed_dir:Path, columns:list=None, valid_ratio:float = valid_ratio ):\n",
    "    total_rows = 0\n",
    "    ext        = \"*.gzip\" #\"*.csv\"\n",
    "    files      = list( compressed_dir.glob(ext) )\n",
    "    #files     = list( compressed_dir.glob(ext) )[:1]\n",
    "    for f in  progress_bar(files):\n",
    "        print(f\"processing: {f.name}\")\n",
    "        df = read_and_split( f, columns,  valid_ratio)\n",
    "        yield df\n",
    "                \n",
    "        total_rows += df.shape[0]\n",
    "    return df   \n",
    "\n",
    "def draw_images_from_df(df, nb=1000, nrows=8, compressed_drawings=True):\n",
    "    compressed = [df.drawing.iloc[i] for i in np.random.randint(0,len(df),nb) ]\n",
    "    if compressed_drawings:\n",
    "        print(\"reading and converting compressed drawings to images\")\n",
    "        ims = drawings2images(compressed, size, line_width, linetype, compressed_drawings) \n",
    "    else:    \n",
    "        print(\"reading and decompressing images\")\n",
    "        ims = decode_images( compressed )\n",
    "    \n",
    "    print(f\"plot images : {len(ims)}\")\n",
    "    plot_images( ims, cm.Greys, max_rows_cols=nrows )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from core_ai.lib.data import *\n",
    "#create a training file with numericalized words and training vs validation samples\n",
    "def createDatasets():\n",
    "    \n",
    "    #move the vocab file to data_prepared\n",
    "    with vocab_file.open('r') as fp:\n",
    "        vocab = json.load(fp)        \n",
    "        with (data_prepared/vocab_file.name).open('w') as fp:\n",
    "            json.dump(vocab, fp)\n",
    "            \n",
    "    #read all to get the toal lenght        \n",
    "    dfs=[]\n",
    "    n_rows=0\n",
    "    for df in read_and_split_all(data_compressed, columns=[\"word_code\",\"drawing\"], valid_ratio=valid_ratio):\n",
    "        gc.collect()\n",
    "        dfs.append(df)\n",
    "        n_rows += len(df)\n",
    "    \n",
    "    #create datasets consisting of nd.arrays and fill them in\n",
    "    gc.collect()\n",
    "    train_ds = GDDataset( np.empty(n_rows,dtype=np.object), np.empty(n_rows,dtype=np.uint16))\n",
    "    is_train = np.empty(n_rows,np.bool)\n",
    "    \n",
    "    n_rows = 0\n",
    "    for df in dfs:\n",
    "        train_ds.x[n_rows:n_rows+len(df)] = df.drawing.values[:]\n",
    "        train_ds.y[n_rows:n_rows+len(df)] = df.word_code.values[:]\n",
    "        is_train[n_rows:n_rows+len(df)]   = df.train.values[:]\n",
    "        df.drop(labels=[\"train\"], axis=1, inplace=True)\n",
    "        n_rows += len(df)\n",
    "    dfs = None\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    valid_ds = GDDataset( train_ds.x[is_train==False], train_ds.y[is_train==False] )\n",
    "    with valid_ds_file.open(\"wb\") as f:\n",
    "        %time pickle.dump(valid_ds, f, pickle.HIGHEST_PROTOCOL)\n",
    "        valid_ds = None\n",
    "        gc.collect()  \n",
    "        \n",
    "    train_ds.x = train_ds.x[is_train]\n",
    "    train_ds.y = train_ds.y[is_train]\n",
    "    is_train = None\n",
    "    gc.collect()\n",
    "\n",
    "    with train_ds_file.open(\"wb\") as f:\n",
    "        %time pickle.dump(train_ds, f, pickle.HIGHEST_PROTOCOL)\n",
    "        train_ds=None\n",
    "        gc.collect()  \n",
    "\n",
    "createDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in the training phase we would read data as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ds, valid_ds = readDatasets(train_ds_file, valid_ds_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"training dataset  : {len(train_ds.x), len(train_ds.x) }\") \n",
    "print(f\"validation dataset: {len(valid_ds.x), len(valid_ds.x) }\") \n",
    "draw_images_from_dataset(train_ds, size, line_width, linetype, nb=1000, nrows=8, compressed_drawings=True), plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to decompress 1000 images\n",
    "nb, dataset = 1000, train_ds\n",
    "compressed  = dataset.x[ np.random.randint(0,len(dataset.x),nb) ]\n",
    "%time ims   = drawings2images(compressed, size, line_width, linetype, compressed_drawings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
